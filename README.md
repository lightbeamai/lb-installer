# lb-debugging
Lightbeam Cluster Debugging Script

## Run load scripts

```
> cd datasource-scale-scripts
> pipenv shell
> pipenv install
> ./locals.sh
```

NOTE: Once you run load script, restart the api-gateway pod.


## Run fix counter script

NOTE: Before running this script make sure you shutdown respective consumer so that no new data will be processed and then run this script.
```
> cd update_mongo
> make
> kubectl apply -f schema-update.yaml
> kubectl exec -it schema-update bash
> python lb_update_mongo.py --fix_counters true  --datasource_id <DATASOURCE_ID>
```

## Structured data cleanup scripts

**Limitations**

 - If there are entities shared among your structured datasource and other datasources.
 - If there are entities generated by other datasources.

Script location: `struct_data_cleanup/`

Script Options:

```shell
usage: main.py [-h] [--datasource_id DATASOURCE_ID]
               [--cmd {delete_entities,delete_cluster,fix_counters,all}]
               [--user_entity_ids_file USER_ENTITY_IDS_FILE]
               [--delete_all_entities DELETE_ALL_ENTITIES]

optional arguments:
  -h, --help            show this help message and exit
  --datasource_id DATASOURCE_ID (Required)
                        Datasource ID.
  --cmd {delete_entities,delete_cluster,fix_counters,all}  (Required)
                        Command to execute.
  --user_entity_ids_file USER_ENTITY_IDS_FILE
                        File name of user entity IDs file.
  --delete_all_entities DELETE_ALL_ENTITIES
                        Whether to delete all entities.
```

The script provides the following commands to execute.
1. `delete_entities`: This command cleans up all the entities, PII record and attribute instances created by the datasource.
   - Make sure to provide `delete_all_entities=true` to delete entities from the `user_entity` collection in MongoDB and the user_entity Elasticsearch collection.
   - `user_entity_ids_file` option is for providing an existing file containing user_entity IDs. This is useful if the command fails somewhere and you need to resume deletion of entities.
2. `delete_cluster`: This command deletes all the table clusters and their respective counters. This will automatically
re-compute counters for tables, databases, schemas, columns etc. **Never run delete_cluster command before running delete_entities if you have a large number of entities**.
3. `fix_counters`: This command re-computes counters for tables, databases, schemas, columns etc and materialises them to MongoDB.

**Instructions to run**
1. The folder contains an example `pod.yaml` file which can be used to launch a pod to do the cleanup.
2. To clean up a datasource, edit the `pod.yaml` on the customer cluster and add the proper arguments to the yaml file. In `containers.args` option, provide the datasource_id, cmd and other options.
Make sure to follow `param_name=value` instead of `param_name value` in args.
3. Once all options are added run `kubectl delete -f pod.yaml && kubectl apply -f pod.yaml`. Tail the logs using `kubectl logs --tail 50000 -f struct-data-cleanup`


## Structured data search and filter

The search and filter operations for structured data are powered by Elasticsearch. We create two indices namely `lb_tables_blue` and `lb_tables_green` and an alias named `lb_tables` pointing to either of the indices. We follow the [blue-green methodology](https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html) here.

The producer and consumer (which is the scanner pod containing two containers named producer and consumer) take care of ingesting the required data into Elasticsearch for search to work properly. The search data ingestion is idempotent. It means that if you have an existing datasource which was running on a version prior to the search/filter feature, you can simply re-run the scanner job and after the job completion, all required data in ES will be present.

As mentioned, ES indices and aliases are critical for the search and filter feature. You can perform the below steps to see if there are any issues with the Elasticsearch setup:

1. Check for both indices are correct mapping
```shell
curl localhost:9200/<index_name>/_mapping
```
Make sure that the commands returns exit code 0. The result should be identical to [latest mapping](https://github.com/lightbeamai/api-spec/blob/v1.1.9.1/src/data/structured_data_tables.json)

The above steps should be repeated for the `lb_table_blue` and `lb_tables_green` indices by replacing the `index_name` placeholder in the command above.

2. Check if the alias is present and is pointing to an index.
```shell
curl -s -XGET localhost:9200/_cat/aliases | grep lb_tables
```
Run the command and see whether it returns one of the indices mentioned above. For example

```shell
curl -s -XGET localhost:9200/_cat/aliases  | grep lb_tables
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   411  100   411    0     0   4140      0 --:--:-- --:--:-- --:--:--  4372
lb_tables                   lb_tables_green
```

3. Check if search works on the alias and both indices.
```shell
curl -H "Content-Type: application/json" -XGET 'http://localhost:9200/<index_or_alias_name>/_search' -d '
{
    "query": {
            "match_all" : {}
    }
}
'
```
If you have some data ingested into ES (via running the scanner job again or registering an instance where the search/filter feature was present), the above command should return some results. If you don't have any data, make sure that the curl request returns a 200 status code.
The above command should be repeated for `lb_tables`, `lb_tables_green`, `lb_tables_blue` by replacing `index_or_alias_name` placeholder.

If all three passes the search/filter should work fine.

## Logs Extraction Script

Description: This script dumps logs of all pods in the given namespace (default is lightbeam) into different files under the directory "logsdump-<date>-<time>" in zipped file from where the script is run.

Usage:

Extract logs:
```shell
./logs-extraction.sh
```

Extract logs with custom namespace:
```shell
./logs-extraction.sh --lb-namespace lightbeam #lightbeam is the namespace here
```

## [CVE Scanner](security_scanning)

## Historical ER
To trigger historical ER, run this command:
```shell
python repairs/trigger_historical_er.py
```

## Doc Analytics Rollup Materialization
To trigger the custom lens rollup materialization for a datasource, run this command:
```shell
python doc_analytics/run_rollup_materialization.py --datasource_id <datasource_id>
```